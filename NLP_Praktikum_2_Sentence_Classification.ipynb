{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1YZrHGyxDJee7HG7Sd0zBkEO03RBsvjtB",
     "timestamp": 1673288265472
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "gpuClass": "standard",
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNpdBBdYjEZ7"
   },
   "source": [
    "# Section 0. Getting Started\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oImy8c20hFkg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674664079623,
     "user_tz": -60,
     "elapsed": 437,
     "user": {
      "displayName": "Tim-Cedric Inhoff",
      "userId": "08463809741907495761"
     }
    },
    "outputId": "54980910-a208-49d4-bb66-f3eefe39a6d5"
   },
   "source": [
    "print('Welcome to the second NLP Praktikum.')"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the second NLP Praktikum.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0.1 Introducing Pytorch\n",
    "\n",
    "[Pytorch](https://pytorch.org/) is a deep learning library support **accelerated tensor operations** (on GPUs) with supoort for **automatic differentiation**.\n",
    "\n",
    "Many of its tensor operation syntax are similar to NumPy, which we worked with last time."
   ],
   "metadata": {
    "id": "NJrUCt8MegLc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Initialize a tensor with the content:\n",
    "# |0  1  2|\n",
    "# |3  4  5|\n",
    "a = torch.Tensor([[0, 1, 2], \n",
    "                  [3, 4, 5]])\n",
    "\n",
    "# Check maxtrix size\n",
    "a.shape"
   ],
   "metadata": {
    "id": "obLkf8Thecrh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674664080056,
     "user_tz": -60,
     "elapsed": 32,
     "user": {
      "displayName": "Tim-Cedric Inhoff",
      "userId": "08463809741907495761"
     }
    },
    "outputId": "c2026b03-1bb3-4be3-dc3c-54c3ed723319"
   },
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Check tensor type, default is float\n",
    "a.dtype"
   ],
   "metadata": {
    "id": "_9FERqHUunFX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674664080057,
     "user_tz": -60,
     "elapsed": 28,
     "user": {
      "displayName": "Tim-Cedric Inhoff",
      "userId": "08463809741907495761"
     }
    },
    "outputId": "cdbbb570-99cf-4592-d9f0-495d9acdc60a"
   },
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float32"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Cast to integer\n",
    "a = a.long()\n",
    "a.dtype"
   ],
   "metadata": {
    "id": "qBjRZSB_u-VF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674664080058,
     "user_tz": -60,
     "elapsed": 24,
     "user": {
      "displayName": "Tim-Cedric Inhoff",
      "userId": "08463809741907495761"
     }
    },
    "outputId": "9c8411d3-c41a-45ea-9866-a9569477392d"
   },
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.int64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Arithmetic operations\n",
    "a.sum()"
   ],
   "metadata": {
    "id": "hdBIM3USfw89",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674664080058,
     "user_tz": -60,
     "elapsed": 20,
     "user": {
      "displayName": "Tim-Cedric Inhoff",
      "userId": "08463809741907495761"
     }
    },
    "outputId": "f1dc39cc-c056-481b-95fd-d946b433fa57"
   },
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(15)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Operation over a certain dimension\n",
    "a.sum(axis=1)"
   ],
   "metadata": {
    "id": "3NNbgRFif61t",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674664080059,
     "user_tz": -60,
     "elapsed": 18,
     "user": {
      "displayName": "Tim-Cedric Inhoff",
      "userId": "08463809741907495761"
     }
    },
    "outputId": "1f182f82-7cb2-4b3f-f685-650f72917d9d"
   },
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 3, 12])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Slicing the matrix (accessing a selection of indices)\n",
    "a[:, 1:a.shape[1]]"
   ],
   "metadata": {
    "id": "yh38AM6RNAsb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674664080060,
     "user_tz": -60,
     "elapsed": 16,
     "user": {
      "displayName": "Tim-Cedric Inhoff",
      "userId": "08463809741907495761"
     }
    },
    "outputId": "45e4b51a-8eb4-4a93-ba1a-e6e46c4260b0"
   },
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2],\n        [4, 5]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Slicing the matrix (accessing a selection of indices)\n",
    "a[:, :-1]"
   ],
   "metadata": {
    "id": "gNHa0FUyNM2w",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674664080060,
     "user_tz": -60,
     "elapsed": 14,
     "user": {
      "displayName": "Tim-Cedric Inhoff",
      "userId": "08463809741907495761"
     }
    },
    "outputId": "e3320a97-bdd5-4833-dbbb-72d59c02ed4b"
   },
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1],\n        [3, 4]])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Extracting content of tensor\n",
    "a[0, 2].item()"
   ],
   "metadata": {
    "id": "vn3NDujer_uH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674664080061,
     "user_tz": -60,
     "elapsed": 12,
     "user": {
      "displayName": "Tim-Cedric Inhoff",
      "userId": "08463809741907495761"
     }
    },
    "outputId": "aeb56048-3c76-4a89-be8a-45e94fabf5f3"
   },
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For more information on PyTorch tensor operations, [this blog](https://jhui.github.io/2018/02/09/PyTorch-Basic-operations/) is a good resource."
   ],
   "metadata": {
    "id": "nT7jGj6SwvfF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0.2 Enabling and testing the GPU\n",
    "\n",
    "Training your model on a GPU is much faster than on CPU.\n",
    "\n",
    "First, let's enable GPUs for this notebook:\n",
    "\n",
    "- Navigate to \"**Edit**\" → \"**Notebook Settings**\"\n",
    "- Select GPU from the **Hardware Accelerator** drop-down\n",
    "\n",
    "Next, we'll check if we can connect to the GPU with PyTorch:"
   ],
   "metadata": {
    "id": "QCmlC-ZbuH_S"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(0)\n",
    "    DEVICE = torch.cuda.current_device()\n",
    "    print('Current device:', torch.cuda.get_device_name(DEVICE))\n",
    "else:\n",
    "    print('Failed to find GPU. Will use CPU.')\n",
    "    DEVICE = 'cpu'"
   ],
   "metadata": {
    "id": "K6ienFJY4W-A",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674664080061,
     "user_tz": -60,
     "elapsed": 11,
     "user": {
      "displayName": "Tim-Cedric Inhoff",
      "userId": "08463809741907495761"
     }
    },
    "outputId": "b806c771-59e1-4836-b055-0ed651cb908c"
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##0.3 Useful Resources\n",
    "\n",
    "The following materials might be useful for solving the tasks in this Praktikum:\n",
    "* Lecture 8, slides 24-28 \n",
    "* Lecture 8, slides 32-41\n",
    "* Lecture 10, slides 32-39"
   ],
   "metadata": {
    "id": "RrdxWjTLEXa0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 1. Intent Classification\n",
    "\n",
    "Many virtual assistants (e.g. Alexa, Google Assistant) rely on **intent classification** models to categorize incoming user requests.\n",
    "\n",
    "We will build a intent classification models based on the **MultiWOZ** dataset.\n",
    "\n",
    "First download and unpack the dataset:"
   ],
   "metadata": {
    "id": "s7xTw8yV3qD7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wget -nc https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.1.zip\n",
    "!unzip -o MultiWOZ_2.1.zip"
   ],
   "metadata": {
    "id": "AIpNVFaR3nVq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674664083667,
     "user_tz": -60,
     "elapsed": 2081,
     "user": {
      "displayName": "Tim-Cedric Inhoff",
      "userId": "08463809741907495761"
     }
    },
    "outputId": "42badc8d-482d-4964-b401-78952e70ecff"
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-25 17:50:09--  https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.1.zip\r\n",
      "Resolving github.com (github.com)... 140.82.121.4\r\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://raw.githubusercontent.com/budzianowski/multiwoz/master/data/MultiWOZ_2.1.zip [following]\r\n",
      "--2023-01-25 17:50:09--  https://raw.githubusercontent.com/budzianowski/multiwoz/master/data/MultiWOZ_2.1.zip\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 20241542 (19M) [application/zip]\r\n",
      "Saving to: ‘MultiWOZ_2.1.zip’\r\n",
      "\r\n",
      "MultiWOZ_2.1.zip    100%[===================>]  19.30M  9.09MB/s    in 2.1s    \r\n",
      "\r\n",
      "2023-01-25 17:50:12 (9.09 MB/s) - ‘MultiWOZ_2.1.zip’ saved [20241542/20241542]\r\n",
      "\r\n",
      "Archive:  MultiWOZ_2.1.zip\r\n",
      "   creating: MultiWOZ_2.1/\r\n",
      "  inflating: MultiWOZ_2.1/train_db.json  \r\n",
      "   creating: __MACOSX/\r\n",
      "   creating: __MACOSX/MultiWOZ_2.1/\r\n",
      "  inflating: __MACOSX/MultiWOZ_2.1/._train_db.json  \r\n",
      "  inflating: MultiWOZ_2.1/testListFile.txt  \r\n",
      "  inflating: __MACOSX/MultiWOZ_2.1/._testListFile.txt  \r\n",
      "  inflating: MultiWOZ_2.1/.DS_Store  \r\n",
      "  inflating: __MACOSX/MultiWOZ_2.1/._.DS_Store  \r\n",
      "  inflating: MultiWOZ_2.1/.README.swp  \r\n",
      "  inflating: __MACOSX/MultiWOZ_2.1/._.README.swp  \r\n",
      "  inflating: MultiWOZ_2.1/slot_descriptions.json  \r\n",
      "  inflating: __MACOSX/MultiWOZ_2.1/._slot_descriptions.json  \r\n",
      "  inflating: MultiWOZ_2.1/police_db.json  \r\n",
      "  inflating: __MACOSX/MultiWOZ_2.1/._police_db.json  \r\n",
      "  inflating: MultiWOZ_2.1/ontology.json  \r\n",
      "  inflating: __MACOSX/MultiWOZ_2.1/._ontology.json  \r\n",
      "  inflating: MultiWOZ_2.1/README     \r\n",
      "  inflating: __MACOSX/MultiWOZ_2.1/._README  \r\n",
      "  inflating: MultiWOZ_2.1/data.json  \r\n",
      "  inflating: MultiWOZ_2.1/taxi_db.json  \r\n",
      "  inflating: __MACOSX/MultiWOZ_2.1/._taxi_db.json  \r\n",
      "  inflating: MultiWOZ_2.1/restaurant_db.json  \r\n",
      "  inflating: __MACOSX/MultiWOZ_2.1/._restaurant_db.json  \r\n",
      "  inflating: MultiWOZ_2.1/hotel_db.json  \r\n",
      "  inflating: __MACOSX/MultiWOZ_2.1/._hotel_db.json  \r\n",
      "  inflating: MultiWOZ_2.1/attraction_db.json  \r\n",
      "  inflating: __MACOSX/MultiWOZ_2.1/._attraction_db.json  \r\n",
      "  inflating: MultiWOZ_2.1/system_acts.json  \r\n",
      "  inflating: __MACOSX/MultiWOZ_2.1/._system_acts.json  \r\n",
      "  inflating: MultiWOZ_2.1/hospital_db.json  \r\n",
      "  inflating: __MACOSX/MultiWOZ_2.1/._hospital_db.json  \r\n",
      "  inflating: MultiWOZ_2.1/tokenization.md  \r\n",
      "  inflating: __MACOSX/MultiWOZ_2.1/._tokenization.md  \r\n",
      "  inflating: MultiWOZ_2.1/valListFile.txt  \r\n",
      "  inflating: __MACOSX/MultiWOZ_2.1/._valListFile.txt  \r\n",
      "  inflating: __MACOSX/._MultiWOZ_2.1  \r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The original data contains conversation threads from **users** and **agents**.\n",
    "\n",
    "We extract the sentences from the **user** side, since that is what the intent classifier needs to handle."
   ],
   "metadata": {
    "id": "clG2-079JUn5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def load_list(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        l = f.readlines()\n",
    "    return {s.strip():1 for s in l}\n",
    "\n",
    "def load_data(data, validfile, testfile):\n",
    "    valid_list, test_list = load_list(validfile), load_list(testfile)\n",
    "    train_set, valid_set, test_set = [], [], []\n",
    "\n",
    "    with open(data) as json_file:\n",
    "        dialogs = json.load(json_file)\n",
    "\n",
    "    for k in dialogs.keys():\n",
    "        if k in test_list:\n",
    "            test_set.append(dialogs[k])\n",
    "        elif k in valid_list:\n",
    "            valid_set.append(dialogs[k])\n",
    "        else:\n",
    "            train_set.append(dialogs[k])\n",
    "\n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "def prepare_data(partition):\n",
    "    input, output = [], []\n",
    "    for d in partition:\n",
    "        # the dialogs are stored in the \"Log\" dictonary entry\n",
    "        for i in range(len(d[\"log\"])):\n",
    "            if (i % 2 == 0):    # only user turns, not the agent's turns\n",
    "                seg = d[\"log\"][i]\n",
    "                # only take turns by the human with annotated dialog_act\n",
    "                if \"dialog_act\" in seg and len(list(seg[\"dialog_act\"].keys())) > 0:\n",
    "                    text = seg[\"text\"]\n",
    "                    input.append(text)\n",
    "                    output.append(list(seg[\"dialog_act\"].keys())[0])\n",
    "    return input, output\n",
    "\n",
    "train, valid, test = load_data(data='MultiWOZ_2.1/data.json',\n",
    "                               validfile='MultiWOZ_2.1/valListFile.txt',\n",
    "                               testfile='MultiWOZ_2.1/testListFile.txt')\n",
    "\n",
    "train_acts = prepare_data(partition=train)\n",
    "valid_acts = prepare_data(partition=valid)\n",
    "test_acts = prepare_data(partition=test)\n",
    "print(\"Done extracting data\")"
   ],
   "metadata": {
    "id": "Nejfhr8F4EXO",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674664089638,
     "user_tz": -60,
     "elapsed": 5979,
     "user": {
      "displayName": "Tim-Cedric Inhoff",
      "userId": "08463809741907495761"
     }
    },
    "outputId": "6dc02b36-42ef-4ce0-9a73-d04203e304fa"
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done extracting data\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take a look at the data:"
   ],
   "metadata": {
    "id": "pySnXzuAmxgq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "show_top_k = 5\n",
    "print(f\"First {show_top_k} sentences:\")\n",
    "print(\"\\n\".join(train_acts[0][:show_top_k]) + \"\\n\")\n",
    "print(f\"First {show_top_k} labels:\")\n",
    "print(\"\\n\".join(train_acts[1][:show_top_k]) + \"\\n\")\n",
    "\n",
    "print(f\"# sent. in train/dev/test: {len(train_acts[0])}, {len(valid_acts[0])}, {len(test_acts[0])}\")"
   ],
   "metadata": {
    "id": "i0lXVdkkFvJA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674664089639,
     "user_tz": -60,
     "elapsed": 20,
     "user": {
      "displayName": "Tim-Cedric Inhoff",
      "userId": "08463809741907495761"
     }
    },
    "outputId": "10a24b29-3169-434f-cd10-2afc2bc90b31"
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 sentences:\n",
      "am looking for a place to to stay that has cheap price range it should be in a type of hotel\n",
      "no, i just need to make sure it's cheap. oh, and i need parking\n",
      "Yes, please. 6 people 3 nights starting on tuesday.\n",
      "how about only 2 nights.\n",
      "No, that will be all. Good bye.\n",
      "\n",
      "First 5 labels:\n",
      "Hotel-Inform\n",
      "Hotel-Inform\n",
      "Hotel-Inform\n",
      "Hotel-Inform\n",
      "general-bye\n",
      "\n",
      "# sent. in train/dev/test: 50836, 6651, 6701\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now extract the **labels**, store them in a dictionary, and create the inverse mapping."
   ],
   "metadata": {
    "id": "khYt3-30pdHJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# preprocess the labels\n",
    "label2id = {}\n",
    "id2label = {}\n",
    "cnt = 0\n",
    "\n",
    "# not assuming having access to test set at this point\n",
    "for l in set(train_acts[1] + valid_acts[1]):\n",
    "    label2id[l] = cnt\n",
    "    id2label[cnt] = l\n",
    "    cnt += 1\n",
    "\n",
    "print(f\"Total number of unique labels: {len(label2id)}\")\n",
    "print(label2id)\n",
    "print(id2label)"
   ],
   "metadata": {
    "id": "EZs0RmdJGVKv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674664089641,
     "user_tz": -60,
     "elapsed": 17,
     "user": {
      "displayName": "Tim-Cedric Inhoff",
      "userId": "08463809741907495761"
     }
    },
    "outputId": "2aa9f3f2-7417-4961-f108-377b157bdbc8"
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique labels: 17\n",
      "{'Restaurant-Request': 0, 'Attraction-Inform': 1, 'Hotel-Inform': 2, 'general-greet': 3, 'Hospital-Inform': 4, 'Hotel-Request': 5, 'Attraction-Request': 6, 'Taxi-Inform': 7, 'Taxi-Request': 8, 'Train-Inform': 9, 'general-bye': 10, 'general-thank': 11, 'Police-Inform': 12, 'Hospital-Request': 13, 'Police-Request': 14, 'Restaurant-Inform': 15, 'Train-Request': 16}\n",
      "{0: 'Restaurant-Request', 1: 'Attraction-Inform', 2: 'Hotel-Inform', 3: 'general-greet', 4: 'Hospital-Inform', 5: 'Hotel-Request', 6: 'Attraction-Request', 7: 'Taxi-Inform', 8: 'Taxi-Request', 9: 'Train-Inform', 10: 'general-bye', 11: 'general-thank', 12: 'Police-Inform', 13: 'Hospital-Request', 14: 'Police-Request', 15: 'Restaurant-Inform', 16: 'Train-Request'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below we first create a vocabulary class.\n",
    "\n",
    "We then fill it with the words in the training data.\n",
    "\n",
    "❓ Why do we need the default token for unknown words in `L7`?"
   ],
   "metadata": {
    "id": "wmF_hm29nF4k"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# preprocess vocabulary\n",
    "class Vocabulary:\n",
    "    def __init__(self, unk=\"<unk>\"):\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.unk_token = unk    # default token for unknown words\n",
    "        self.token2id = {self.unk_token: 0}\n",
    "\n",
    "    def update_vocab(self, sentences):\n",
    "        for sent_id, sent in enumerate(sentences):\n",
    "            tokens = self.tokenizer(sent)\n",
    "            for t in tokens:\n",
    "                if t not in self.token2id:\n",
    "                    self.token2id[t] = len(self.token2id)\n",
    "            # print out first sentences as example\n",
    "            if sent_id < 5:\n",
    "                print(f\"Input: {sent}\")\n",
    "                print(f\"After tokenization: {tokens}\\n\")\n",
    "        print(f\"Vocab size: {len(self.token2id)}\")\n",
    "\n",
    "    def sentence_to_id(self, sentence):\n",
    "        res = []\n",
    "        for t in self.tokenizer(sentence):\n",
    "            if t in self.token2id:\n",
    "                res.append(self.token2id[t])\n",
    "            else:\n",
    "                res.append(self.token2id[self.unk_token])\n",
    "        return res\n",
    "\n",
    "# init empty vocabulary\n",
    "vocab = Vocabulary()\n",
    "# create vocabulary based on training data\n",
    "vocab.update_vocab(train_acts[0])"
   ],
   "metadata": {
    "id": "hHJiSkKXNuPV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674664090158,
     "user_tz": -60,
     "elapsed": 530,
     "user": {
      "displayName": "Tim-Cedric Inhoff",
      "userId": "08463809741907495761"
     }
    },
    "outputId": "6d9629fb-fd9d-4fff-9c2b-d6d1bebfe763"
   },
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: am looking for a place to to stay that has cheap price range it should be in a type of hotel\n",
      "After tokenization: ['am', 'looking', 'for', 'a', 'place', 'to', 'to', 'stay', 'that', 'has', 'cheap', 'price', 'range', 'it', 'should', 'be', 'in', 'a', 'type', 'of', 'hotel']\n",
      "\n",
      "Input: no, i just need to make sure it's cheap. oh, and i need parking\n",
      "After tokenization: ['no', ',', 'i', 'just', 'need', 'to', 'make', 'sure', 'it', \"'\", 's', 'cheap', '.', 'oh', ',', 'and', 'i', 'need', 'parking']\n",
      "\n",
      "Input: Yes, please. 6 people 3 nights starting on tuesday.\n",
      "After tokenization: ['yes', ',', 'please', '.', '6', 'people', '3', 'nights', 'starting', 'on', 'tuesday', '.']\n",
      "\n",
      "Input: how about only 2 nights.\n",
      "After tokenization: ['how', 'about', 'only', '2', 'nights', '.']\n",
      "\n",
      "Input: No, that will be all. Good bye.\n",
      "After tokenization: ['no', ',', 'that', 'will', 'be', 'all', '.', 'good', 'bye', '.']\n",
      "\n",
      "Vocab size: 3596\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "❓ What do you observe when observing the following example?"
   ],
   "metadata": {
    "id": "ISJQyxeqn3tk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(vocab.sentence_to_id(\"I'm looking for a hotel this Thursday in Karlsruhe\"))"
   ],
   "metadata": {
    "id": "MoU1Zhrznd8u",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674664090159,
     "user_tz": -60,
     "elapsed": 6,
     "user": {
      "displayName": "Tim-Cedric Inhoff",
      "userId": "08463809741907495761"
     }
    },
    "outputId": "4ca79f59-2e4e-4b45-bf75-3e06dcfb6cf4"
   },
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 27, 76, 2, 3, 4, 19, 246, 291, 16, 0]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To prepare for training, we tensorize the train, dev, test sets."
   ],
   "metadata": {
    "id": "al6RBzLtoMlf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset, valid_dataset, test_dataset = [], [], []\n",
    "\n",
    "def convert_to_tensor(text, label):\n",
    "    text_tensor = torch.Tensor(vocab.sentence_to_id(text)).long().unsqueeze(0).to(DEVICE)\n",
    "    label_tensor = torch.Tensor([label2id[label]]).long().to(DEVICE)\n",
    "    return (text_tensor, label_tensor)\n",
    "\n",
    "# train\n",
    "for sent_id, sent in enumerate(train_acts[0]):\n",
    "    train_dataset.append(convert_to_tensor(sent, train_acts[1][sent_id]))    \n",
    "print(\"First instance in training: \", train_dataset[0])\n",
    "\n",
    "# dev\n",
    "for sent_id, sent in enumerate(valid_acts[0]):\n",
    "    valid_dataset.append(convert_to_tensor(sent, valid_acts[1][sent_id]))\n",
    "print(\"First instance in dev: \", valid_dataset[0])\n",
    "\n",
    "# test\n",
    "for sent_id, sent in enumerate(test_acts[0]):\n",
    "    test_dataset.append(convert_to_tensor(sent, test_acts[1][sent_id]))"
   ],
   "metadata": {
    "id": "zaSh3Pb9S0I8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674664094996,
     "user_tz": -60,
     "elapsed": 4841,
     "user": {
      "displayName": "Tim-Cedric Inhoff",
      "userId": "08463809741907495761"
     }
    },
    "outputId": "f7696fa5-444b-4c39-fa37-8d593642b577"
   },
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First instance in training:  (tensor([[ 1,  2,  3,  4,  5,  6,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  4,\n",
      "         17, 18, 19]], device='cuda:0'), tensor([2], device='cuda:0'))\n",
      "First instance in dev:  (tensor([[ 22,  24,   6, 108,   4,  19,  16,  61, 114,   8,   9, 104, 105,  29]],\n",
      "       device='cuda:0'), tensor([2], device='cuda:0'))\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 2. Training Intent Classifiers\n",
    "\n",
    "Below is an implementation of a **bag-of-words** classifier.\n",
    "\n",
    "❓ Which parameters does it have, and what are their shapes? "
   ],
   "metadata": {
    "id": "xJAMJjf2N65K"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWordsClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(BagOfWordsClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text):\n",
    "        \"\"\" Forward pass, return a (log) prob. distribution over all labels. \n",
    "        Arguments: \n",
    "        text: input sentence represented as a sequence of token IDs;\n",
    "        shape B x T (batch size x sequence length)\n",
    "        In this exercise we always use batch size of 1.\n",
    "        \"\"\"\n",
    "        # shape: B x H (embedding dimension)\n",
    "        embedded = self.embedding(text)\n",
    "        embedded_sum = embedded[:, 0]\n",
    "        for i in range(1, embedded.size(dim=1)):\n",
    "            embedded_sum += embedded[:, i]\n",
    "        # shape: B x L (number of labels)\n",
    "        fc_out = self.fc(embedded_sum / embedded.size(dim=1))\n",
    "        return F.log_softmax(fc_out, dim=-1)    # B x L"
   ],
   "metadata": {
    "id": "QOBjZuuCJo_4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674664336366,
     "user_tz": -60,
     "elapsed": 6,
     "user": {
      "displayName": "Tim-Cedric Inhoff",
      "userId": "08463809741907495761"
     }
    }
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        return self.fc(hidden)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below is a `Trainer` class.\n",
    "\n",
    "Please go over the `train_one_epoch` method once thoroughly."
   ],
   "metadata": {
    "id": "3XLm_J1ovLaT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer,\n",
    "                 train_dataset, valid_dataset, test_dataset):\n",
    "        print(\"===== Model parameters and shape ====\")\n",
    "        for name, param in model.named_parameters():\n",
    "            print(name, param.shape)\n",
    "\n",
    "        self.criterion = torch.nn.NLLLoss()\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dataset, self.valid_dataset = train_dataset, valid_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "        self.bsz = 64   # mini-batch size\n",
    "        self.log_interval = 10000\n",
    "\n",
    "    def train_one_epoch(self, dataset, epoch):\n",
    "        random.shuffle(dataset) # shuffle dataset\n",
    "        model.train()   # set model to training mode\n",
    "        total_acc, total_count = 0, 0\n",
    "\n",
    "        for idx, (text, label) in enumerate(dataset):\n",
    "            predicted_label = model(text)   # forward pass\n",
    "            loss = self.criterion(predicted_label, label)    # loss calculation\n",
    "            loss.backward() # backward pass, accmulate gradient\n",
    "\n",
    "            if (idx + 1) % self.bsz == 0:\n",
    "                optimizer.step()     # update model parameters\n",
    "                optimizer.zero_grad()   # clear gradients from graph\n",
    "\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "\n",
    "            if idx % self.log_interval == 0 and idx > 0: # print logging\n",
    "                print(f\"| epoch {epoch:3d} | {idx:5d}/{len(dataset):5d} sentences | training accuracy {total_acc/total_count:.3f}\")\n",
    "                total_acc, total_count = 0, 0\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "        model.eval()    # set model to eval mode\n",
    "        total_acc, total_count = 0, 0\n",
    "\n",
    "        with torch.no_grad():   # no gradient accmulation\n",
    "            for idx, (text, label) in enumerate(dataset):\n",
    "                predicted_label = model(text).argmax(1).item()\n",
    "                if predicted_label == label.item():\n",
    "                    total_acc += 1\n",
    "                # print examples of wrong predictions:\n",
    "                elif random.random() < 1.0 / len(dataset):\n",
    "                    print(\"sentence:\\t\", valid_acts[0][idx])\n",
    "                    print(\"predicted:\\t\", id2label[predicted_label])\n",
    "                    print(\"true label:\\t\", id2label[label.item()], \"\\n\")\n",
    "                total_count += 1\n",
    "        return total_acc / total_count\n",
    "\n",
    "    def train(self, num_epoch):\n",
    "        for i in range(num_epoch):\n",
    "            self.train_one_epoch(self.train_dataset, i)\n",
    "            dev_accu = self.evaluate(self.valid_dataset)\n",
    "            print(f\"Dev accuracy after epoch {i}: {dev_accu:.2f} \\n\\n\")\n",
    "\n",
    "        # done training, save model\n",
    "        test_accu = self.evaluate(self.test_dataset)\n",
    "        print(f\"Test accuracy {test_accu:.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "num_class = len(label2id)\n",
    "vocab_size = len(vocab.token2id)\n",
    "emb_dim = 300   # embedding dimension\n",
    "n_layers = 4\n",
    "dropout = 0.2\n",
    "\n",
    "# init model and optimizer\n",
    "model = TextClassifier(vocab_size=vocab_size, embedding_dim=emb_dim, hidden_dim=300, output_dim=num_class).to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# init trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  train_dataset=train_dataset,\n",
    "                  valid_dataset=valid_dataset,\n",
    "                  test_dataset=test_dataset)\n",
    "# start training, train for 3 epochs (going through the dataset 3 times)\n",
    "trainer.train(num_epoch=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After starting the training, you will see the performance is quite poor.\n",
    "\n",
    "This is the starting point of Implementation Task 1 in Section 3.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Model parameters and shape ====\n",
      "embedding.weight torch.Size([3596, 300])\n",
      "fc.weight torch.Size([17, 300])\n",
      "fc.bias torch.Size([17])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 16\u001B[0m\n\u001B[1;32m     10\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     11\u001B[0m                   optimizer\u001B[38;5;241m=\u001B[39moptimizer,\n\u001B[1;32m     12\u001B[0m                   train_dataset\u001B[38;5;241m=\u001B[39mtrain_dataset,\n\u001B[1;32m     13\u001B[0m                   valid_dataset\u001B[38;5;241m=\u001B[39mvalid_dataset,\n\u001B[1;32m     14\u001B[0m                   test_dataset\u001B[38;5;241m=\u001B[39mtest_dataset)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# start training, train for 3 epochs (going through the dataset 3 times)\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[34], line 59\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, num_epoch)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(\u001B[38;5;28mself\u001B[39m, num_epoch):\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epoch):\n\u001B[0;32m---> 59\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m         dev_accu \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalid_dataset)\n\u001B[1;32m     61\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDev accuracy after epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdev_accu\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[34], line 27\u001B[0m, in \u001B[0;36mTrainer.train_one_epoch\u001B[0;34m(self, dataset, epoch)\u001B[0m\n\u001B[1;32m     25\u001B[0m predicted_label \u001B[38;5;241m=\u001B[39m model(text)   \u001B[38;5;66;03m# forward pass\u001B[39;00m\n\u001B[1;32m     26\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion(predicted_label, label)    \u001B[38;5;66;03m# loss calculation\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# backward pass, accmulate gradient\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (idx \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbsz \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     30\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()     \u001B[38;5;66;03m# update model parameters\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP_Vl/venv/lib/python3.10/site-packages/torch/_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    487\u001B[0m     )\n\u001B[0;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP_Vl/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "num_class = len(label2id)\n",
    "vocab_size = len(vocab.token2id)\n",
    "emb_dim = 300   # embedding dimension\n",
    "\n",
    "# init model and optimizer\n",
    "model = BagOfWordsClassifier(vocab_size, emb_dim, num_class).to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# init trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  train_dataset=train_dataset,\n",
    "                  valid_dataset=valid_dataset,\n",
    "                  test_dataset=test_dataset)\n",
    "# start training, train for 3 epochs (going through the dataset 3 times)\n",
    "trainer.train(num_epoch=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 3. Tasks"
   ],
   "metadata": {
    "id": "1SHSs71zWMGU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementation Task 1: Fixing Bag-of-Words Classifier\n",
    "Currently, the bag-of-word classifier is **not** implemented **correctly**:\n",
    "It only uses the embedding of the **first word** for prediction.\n",
    "\n",
    "Implement an aggregation mechenism and report the perfromance.\n",
    "\n"
   ],
   "metadata": {
    "id": "lsKwrBY8FB5i"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "embedded\n",
    "## Implementation Task 2: Sequential Layer\n",
    "\n",
    "Recall that the bag-of-word approach **ignores word orders**. \n",
    "\n",
    "In the lecture, we cover several models that are better at modeling the **sequential** nature of the input.\n",
    "\n",
    "Implement an **LSTM-based** classifier, and compare the performance to the previous results.\n",
    "\n",
    "(*Hint: see [here](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) for the documentation of LSTM in PyTorch*)\n"
   ],
   "metadata": {
    "id": "5wiau9BINtzp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Implementation Task 3: Low-Resource Condition\n",
    "Currently, there are over **50,000 sentences** in our training data. \n",
    "\n",
    "In many practical scenarios, we do not have so much **labeled data** for training.\n",
    "\n",
    "Truncate the training data to simulate the **low-resource** case of training with 1,000 sentences. \n",
    "\n",
    "How do the results differ compared to previously?\n",
    "\n",
    "What could we do to improve the system? Implment this idea."
   ],
   "metadata": {
    "id": "gMXEAlF5NwAM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Next Steps\n",
    "\n",
    "This Praktikum is ungraded. You do not have to send your solutions in.\n",
    "\n",
    "You are, however, highly encouraged to ask for feedback during the practical session."
   ],
   "metadata": {
    "id": "g161uqc3SMjC"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxy3Fsmtk64C"
   },
   "source": [
    "#Credits\n",
    "\n",
    "Some code snippets in this Praktikum are from\n",
    "[pytorch](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html).\n",
    "\n",
    "The MultiWOZ dataset we worked with is created by [Budzianowski et al. (2018)](https://aclanthology.org/D18-1547.pdf)."
   ]
  }
 ]
}
